{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0677302b",
   "metadata": {},
   "source": [
    "### Introduction:\n",
    "In this project, I used logistic regression to classify handwritten digits using the dataset available in scikit-learn, known as the load_digits dataset. The dataset contains images of handwritten digits (0-9), making it a suitable benchmark for image classification tasks. By applying logistic regression, a foundational classification algorithm, I aimed to accurately predict the digit represented in each image based on its pixel values.\n",
    "\n",
    "### Objective:\n",
    "The primary objective of this project is to develop a logistic regression model capable of accurately classifying handwritten digits from the load_digits dataset. Through this project, I seek to demonstrate proficiency in image classification tasks and highlight the effectiveness of logistic regression in real-world scenarios.\n",
    "\n",
    "### Methodologies:\n",
    "\n",
    "##### 1. Data Acquisition: \n",
    "I utilized the load_digits() function from the sklearn.datasets module to load the load_digits dataset, which contains images of handwritten digits along with their corresponding labels.\n",
    "\n",
    "##### 2. Exploratory Data Analysis: \n",
    "Before model development, I explored the dataset to understand its structure and characteristics. Visualization techniques were employed to gain insights into the distribution of digits and the appearance of handwritten images.\n",
    "\n",
    "##### 3. Data Preprocessing: \n",
    "The images were flattened to convert them into 1D arrays, suitable for logistic regression. Additionally, the dataset was split into training and testing sets to facilitate model training and evaluation.\n",
    "\n",
    "##### 4. Model Development: \n",
    "A logistic regression model was constructed using the LogisticRegression class from sklearn.linear_model. The model was trained on the training data using the fit() method.\n",
    "\n",
    "##### 5. Model Evaluation: \n",
    "Following model training, predictions were made on the testing data using the predict() method. Performance metrics such as accuracy score and classification report were computed to assess the model's effectiveness in digit classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c92ef683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "156925fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "44315d59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DESCR', 'data', 'feature_names', 'frame', 'images', 'target', 'target_names']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# explore the dataset\n",
    "dir(digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dc91a023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  5., 13.,  9.,  1.,  0.,  0.,  0.,  0., 13., 15., 10.,\n",
       "       15.,  5.,  0.,  0.,  3., 15.,  2.,  0., 11.,  8.,  0.,  0.,  4.,\n",
       "       12.,  0.,  0.,  8.,  8.,  0.,  0.,  5.,  8.,  0.,  0.,  9.,  8.,\n",
       "        0.,  0.,  4., 11.,  0.,  1., 12.,  7.,  0.,  0.,  2., 14.,  5.,\n",
       "       10., 12.,  0.,  0.,  0.,  0.,  6., 13., 10.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can view first five rows\n",
    "digits.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b65c7301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let us visualize the first 5 images\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "for i in range(5):\n",
    "    plt.subplot(1, 5, i + 1)\n",
    "    plt.imshow(digits.images[i])\n",
    "    plt.title(f\"Digit: {digits.target[i]}\")\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7cee8f96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let us see the first five target variables\n",
    "digits.target[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41439209",
   "metadata": {},
   "source": [
    "We can see that the first five images correspond to the first five targets, which are 0 to 5.\n",
    "So, remember, we have a dataset with different numbers and we are to predict which of them\n",
    "correspond to the target.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12836f5",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dfceb9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into features(X) and target (y) variables\n",
    "X = digits.data\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "99410b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "06780019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the model\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9d989b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "eabe3557",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e2e0ac0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Model Evaluation on training data\n",
    "train_accuracy = model.score(X_train, y_train)\n",
    "print(\"Training Accuracy:\", train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "15919f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's predict using our X_test\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "dd4b43fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9694444444444444\n"
     ]
    }
   ],
   "source": [
    "# Compute accuracy manually\n",
    "train_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Training Accuracy:\", train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "907357a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        33\n",
      "           1       0.97      1.00      0.98        28\n",
      "           2       0.97      1.00      0.99        33\n",
      "           3       0.97      0.97      0.97        34\n",
      "           4       1.00      0.96      0.98        46\n",
      "           5       0.92      0.94      0.93        47\n",
      "           6       0.94      0.97      0.96        35\n",
      "           7       1.00      0.97      0.99        34\n",
      "           8       0.97      0.97      0.97        30\n",
      "           9       0.97      0.95      0.96        40\n",
      "\n",
      "    accuracy                           0.97       360\n",
      "   macro avg       0.97      0.97      0.97       360\n",
      "weighted avg       0.97      0.97      0.97       360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# classification report\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0c6d8a",
   "metadata": {},
   "source": [
    "### Results:\n",
    "The logistic regression model achieved an accuracy of 97% on the testing data, demonstrating its capability to accurately classify handwritten digits from the load_digits dataset. The classification report provided detailed insights into the model's precision, recall, and F1-score for each digit class, indicating satisfactory performance across all classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844cb785",
   "metadata": {},
   "source": [
    "### Saving the model\n",
    "Since the model is working perfectly, it's time to save it so that your team members can easily call it up to make predictions. To do this, we import the joblib library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9e29afd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8eefa996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['image_class']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saving the model\n",
    "joblib.dump(model, 'image_class')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4ec157",
   "metadata": {},
   "source": [
    "### What if we want to use the model?\n",
    "This can be done by loading the model on our notebook just like you would a dataset but this time, from the joblib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "64f7be2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model and save it to a variable\n",
    "model_use = joblib.load('image_class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "aca6ed48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can then predict using this\n",
    "model_use.predict([X[8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e26dcf2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# more predictions\n",
    "model_use.predict(X[0:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7353d329",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "In conclusion, this project showcased the successful implementation of logistic regression for handwritten digit classification using the load_digits dataset. By leveraging fundamental machine learning concepts and thorough data analysis, I developed a reliable model capable of accurately predicting the digits represented in handwritten images. This project serves as a testament to my proficiency in image classification tasks and my ability to apply machine learning algorithms to real-world datasets.\n",
    "\n",
    "### Future Directions:\n",
    "Moving forward, I plan to explore advanced image classification techniques, such as convolutional neural networks (CNNs), to further enhance model performance. Additionally, I aim to deploy the model as a web application or integrate it into other projects for practical applications in optical character recognition (OCR) and digit recognition systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
